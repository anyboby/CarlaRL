{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomUniform, normal\n",
    "from keras.layers import Dense, Conv1D, Conv2D, Flatten, Input, MaxPool2D, concatenate, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the racing game as environment for the Reinforcement Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Game Setup ######### \n",
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code displays:\n",
    "- states: 96 pixles x 96 pixles x RGB\n",
    "- actions (3 possibilities):\n",
    "\n",
    "[1, 0, 0] = Right % (steering)\n",
    "\n",
    "[-1, 0, 0] = Left % (steering)\n",
    "\n",
    "[0, 1, 0] = Straight % speed\n",
    "\n",
    "[0, 0, 1] = Brake % speed (calculated against speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of params affecting the environment: (96, 96, 3)\n",
      "No of possible actions: Box(3,)\n",
      "Example for random action [ 0.51571882  0.83721125  0.28387302]\n",
      "GPUs:  []\n"
     ]
    }
   ],
   "source": [
    "########### Game and Environment info ###########\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space\n",
    "random_action = env.action_space.sample()\n",
    "print('No of params affecting the environment:', states)\n",
    "print('No of possible actions:', actions)\n",
    "print('Example for random action', random_action)\n",
    "\n",
    "\n",
    "########### Check if Keras uses GPU ##########\n",
    "from tensorflow.python.client import device_lib\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "#print(device_lib.list_local_devices())\n",
    "print(\"GPUs: \", K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /fzi/ids/rottach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fzi/ids/rottach/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /fzi/ids/rottach/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 96, 96, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 89, 89, 8)    520         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 44, 44, 8)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 37, 37, 16)   8208        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 18, 18, 16)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 5184)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           331840      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          8320        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            129         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            129         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3)            0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 349,787\n",
      "Trainable params: 349,531\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Actor:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        # Actual model\n",
    "        self.model = self._build_model()\n",
    "        # Target net\n",
    "        self.target_model = self._build_model()\n",
    "        self.adam_optimizer = self.optimizer()\n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        \n",
    "        state = Input((self.state_size))\n",
    "        # Convolutions\n",
    "        x = Conv2D(8, kernel_size=8, activation='relu', input_shape=self.state_size)(state)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(16, kernel_size=8, activation='relu') (x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        \n",
    "        # Connect convolution and dense layers\n",
    "        # 2D -> 1D (Linearization)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        # 3 hidden layers\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        # Creates 512 x 512 weights\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        # Defining the output for each dimension seperately\n",
    "        steering = Dense(1,activation='tanh',kernel_initializer=RandomUniform(minval=-0.05,maxval=0.05))(x)   \n",
    "        acceleration = Dense(1,activation='sigmoid',kernel_initializer=RandomUniform(minval=-0.05,maxval=0.05))(x)   \n",
    "        brake = Dense(1,activation='sigmoid',kernel_initializer=RandomUniform(minval=-0.05,maxval=0.05))(x) \n",
    "        out = concatenate([steering,acceleration,brake],axis=-1)\n",
    "        \n",
    "        model = Model(input=state,output=out)        \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def predict(self, state):\n",
    "        \"\"\" Prediction of actor network\n",
    "        \"\"\"        \n",
    "        action = self.model.predict(np.expand_dims(state, axis=0))\n",
    "        # Normalize the steering between -1 and 1\n",
    "        # Only used if sigmoid function\n",
    "        # action[0] = (action[0] * 2) - 1; \n",
    "        return action\n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Prediction of target network\n",
    "        \"\"\"\n",
    "        return self.target_model.predict(inp)\n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer model weights to target model with a factor of Tau\n",
    "        \"\"\"\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1 - self.tau)* target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    def train(self, states, actions, grads):\n",
    "        \"\"\" Actor Training\n",
    "        \"\"\"\n",
    "        self.adam_optimizer([states, grads])\n",
    "    def optimizer(self):\n",
    "        \"\"\" Actor Optimizer\n",
    "        \"\"\"\n",
    "        action_gdts = K.placeholder(shape=(None, self.action_size))\n",
    "        params_grad = tf.gradients(self.model.output, self.model.trainable_weights, -action_gdts)\n",
    "        grads = zip(params_grad, self.model.trainable_weights)\n",
    "        return K.function([self.model.input, action_gdts], [tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)][1:])\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path + '_actor.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)\n",
    "actor = Actor((96,96,1), 3, 0.001, 0.1)\n",
    "actor.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 96, 96, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 88, 88, 8)    656         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 44, 44, 8)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 36, 36, 16)   10384       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 18, 18, 16)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 18, 18, 16)   64          max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 5184)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          663680      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 131)          0           dense_11[0][0]                   \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          16896       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 691,809\n",
      "Trainable params: 691,777\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Critic:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        # Actual model\n",
    "        self.model = self._build_model()\n",
    "        self.model.compile(Adam(self.learning_rate), 'mse')\n",
    "        # Target net for stability\n",
    "        self.target_model = self._build_model()        \n",
    "        self.target_model.compile(Adam(self.learning_rate), 'mse')\n",
    "        # Function to compute Q-value gradients (Actor Optimization)\n",
    "        self.action_grads = K.function([self.model.input[0], self.model.input[1]], K.gradients(self.model.output, [self.model.input[1]]))\n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        state = Input((self.state_size))\n",
    "        x = Conv2D(8, kernel_size=9, activation='relu', input_shape=self.state_size)(state)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(16, kernel_size=9, activation='relu')(x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        # Actions\n",
    "        action_shape = (self.action_size,)\n",
    "        action_layer = Input(shape=action_shape)\n",
    "        \n",
    "        # TODO: In the original paper the actions are merged in the second hidden layer\n",
    "        x = Flatten()(x)\n",
    "        x = concatenate([Dense(128, activation='relu')(x), action_layer])\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        out = Dense(1, activation='linear', kernel_initializer=RandomUniform(minval=-0.05,maxval=0.05))(x)\n",
    "        return Model([state, action_layer], out)\n",
    "    \n",
    "    def gradients(self, states, actions):\n",
    "        \"\"\" Compute Q-value gradients w.r.t. states and policy-actions\n",
    "        \"\"\"\n",
    "        return self.action_grads([states, actions])\n",
    "    \n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Prediction of target network\n",
    "        \"\"\"\n",
    "        return self.target_model.predict(inp)\n",
    "    # Why does the Critic have no predict function\n",
    "    \n",
    "    def train_on_batch(self, states, actions, critic_target):\n",
    "        \"\"\" Train the critic network on batch of sampled experience\n",
    "            using the keras function train_on_batch\n",
    "        \"\"\"\n",
    "        return self.model.train_on_batch([states, actions], critic_target)\n",
    "    \n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer model weights to target model with a factor of Tau\n",
    "        \"\"\"\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1 - self.tau)* target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path + '_critic.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)\n",
    "         \n",
    "critic = Critic((96,96,1), 3, 0.001, 0.1)\n",
    "critic.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Utils\n",
    "import numpy\n",
    "\n",
    "\"\"\" Original Code by @jaara: https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "\"\"\"\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Utils\n",
    "from collections import deque\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    \"\"\" Memory Buffer Helper class for Experience Replay\n",
    "    using a double-ended queue or a Sum Tree (for PER)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size, with_per = False):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        if(with_per):\n",
    "            # Prioritized Experience Replay\n",
    "            self.alpha = 0.5\n",
    "            self.epsilon = 0.01\n",
    "            self.buffer = SumTree(buffer_size)\n",
    "        else:\n",
    "            # Standard Buffer\n",
    "            self.buffer = deque()\n",
    "        self.count = 0\n",
    "        self.with_per = with_per\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state, error=None):\n",
    "        \"\"\" Save an experience to memory, optionally with its TD-Error\n",
    "        \"\"\"\n",
    "\n",
    "        experience = (state, action, reward, done, new_state)\n",
    "        if(self.with_per):\n",
    "            priority = self.priority(error[0])\n",
    "            self.buffer.add(priority, experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            # Check if buffer is already full\n",
    "            if self.count < self.buffer_size:\n",
    "                self.buffer.append(experience)\n",
    "                self.count += 1\n",
    "            else:\n",
    "                self.buffer.popleft()\n",
    "                self.buffer.append(experience)\n",
    "\n",
    "    def priority(self, error):\n",
    "        \"\"\" Compute an experience priority, as per Schaul et al.\n",
    "        \"\"\"\n",
    "        return (error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\" Current Buffer Occupation\n",
    "        \"\"\"\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Sample a batch, optionally with (PER)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "\n",
    "        # Sample using prorities\n",
    "        if(self.with_per):\n",
    "            T = self.buffer.total() // batch_size\n",
    "            for i in range(batch_size):\n",
    "                a, b = T * i, T * (i + 1)\n",
    "                s = random.uniform(a, b)\n",
    "                idx, error, data = self.buffer.get(s)\n",
    "                batch.append((*data, idx))\n",
    "            idx = np.array([i[5] for i in batch])\n",
    "        # Sample randomly from Buffer\n",
    "        elif self.count < batch_size:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Return a batch of experience\n",
    "        s_batch = np.array([i[0] for i in batch])\n",
    "        a_batch = np.array([i[1] for i in batch])\n",
    "        r_batch = np.array([i[2] for i in batch])\n",
    "        d_batch = np.array([i[3] for i in batch])\n",
    "        new_s_batch = np.array([i[4] for i in batch])\n",
    "        return s_batch, a_batch, r_batch, d_batch, new_s_batch, idx\n",
    "\n",
    "    def update(self, idx, new_error):\n",
    "        \"\"\" Update priority for idx (PER)\n",
    "        \"\"\"\n",
    "        self.buffer.update(idx, self.priority(new_error))\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\" Clear buffer / Sum Tree\n",
    "        \"\"\"\n",
    "        if(self.with_per): self.buffer = SumTree(buffer_size)\n",
    "        else: self.buffer = deque()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \"\"\" Deep Deterministic Policy Gradient (DDPG) Helper Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, batch_no):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        # Environment and parameters\n",
    "        self.state_size = (batch_no,) + state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        # Create actor and critic networks\n",
    "        self.actor = Actor(state_size, self.action_size, 0.1 * self.learning_rate, 0.001)\n",
    "        self.critic = Critic(state_size, self.action_size, self.learning_rate, 0.001)\n",
    "        self.buffer = MemoryBuffer(10000)\n",
    "        self.steps = 1001\n",
    "        self.noise_episodes = 1000 * 0.2\n",
    "\n",
    "    def policy_action(self, s):\n",
    "        \"\"\" Use the actor to predict value\n",
    "        \"\"\"\n",
    "        return self.actor.predict(s)[0]\n",
    "\n",
    "    def bellman(self, rewards, q_values, dones):\n",
    "        \"\"\" Use the Bellman Equation to compute the critic target\n",
    "        \"\"\"\n",
    "        critic_target = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                critic_target[i] = rewards[i]\n",
    "            else:\n",
    "                critic_target[i] = rewards[i] + self.gamma * q_values[i]\n",
    "        return critic_target\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state):\n",
    "        \"\"\" Store experience in memory buffer\n",
    "        \"\"\"\n",
    "        self.buffer.memorize(state, action, reward, done, new_state)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return self.buffer.sample_batch(batch_size)\n",
    "\n",
    "    def update_models(self, states, actions, critic_target):\n",
    "        \"\"\" Update actor and critic networks from sampled experience\n",
    "        \"\"\"\n",
    "        # Train critic\n",
    "        self.critic.train_on_batch(states, actions, critic_target)\n",
    "        # Q-Value Gradients under Current Policy\n",
    "        actions = self.actor.model.predict(states)\n",
    "        grads = self.critic.gradients(states, actions)\n",
    "        # Train actor\n",
    "        self.actor.train(states, actions, np.array(grads).reshape((-1, self.action_size)))\n",
    "        # Transfer weights to target networks at rate Tau\n",
    "        self.actor.transfer_weights()\n",
    "        self.critic.transfer_weights()\n",
    "\n",
    "    def train(self, env, render, batch_size, nb_episodes):\n",
    "        results = []\n",
    "\n",
    "        # First, gather experience\n",
    "        # tqdm_e = tqdm(range(nb_episodes), desc='Score', leave=True, unit=\" episodes\")\n",
    "        f = open('results.txt', 'r+')\n",
    "        f.truncate(0)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "        for e in range(nb_episodes):\n",
    "            start = time.time()\n",
    "            with open(\"results.txt\", \"a\") as myfile:\n",
    "                myfile.write(\"***************** Episode: {}\".format(e))\n",
    "            # Reset episode\n",
    "            time_step, cumul_reward, done = 0, 0, False\n",
    "            old_state = env.reset()\n",
    "            actions, states, rewards = [], [], []\n",
    "            noise = OrnsteinUhlenbeckProcess(size=self.action_size, theta=.15, mu=0., sigma=.3)\n",
    "\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            old_state = cv2.cvtColor(old_state, cv2.COLOR_BGR2GRAY)\n",
    "            old_state = old_state.reshape(old_state.shape[0], old_state.shape[1] , 1)\n",
    "            \n",
    "            for step in range(self.steps):\n",
    "                if render:\n",
    "                    env.render()                   \n",
    "                # Actor picks an action (following the deterministic policy)\n",
    "                a = self.policy_action(old_state)\n",
    "                \n",
    "                if e < self.noise_episodes:\n",
    "                    a = np.clip(a+noise.sample(), -1, 1)\n",
    "                new_state, r, done, _ = env.step(a)\n",
    "                \n",
    "                # Reshape new state\n",
    "                new_state = cv2.cvtColor(new_state, cv2.COLOR_BGR2GRAY)\n",
    "                new_state = new_state.reshape(new_state.shape[0], new_state.shape[1] , 1)\n",
    "                # Append to replay buffer\n",
    "                self.memorize(old_state, a, r, done, new_state)\n",
    "                if self.buffer.count > batch_size:\n",
    "                    states, actions, rewards, dones, new_states, _ = self.sample_batch(batch_size)\n",
    "                    q_values = self.critic.target_predict([new_states, self.actor.target_predict(new_states)])\n",
    "                    critic_target = self.bellman(rewards, q_values, dones)\n",
    "                old_state = new_state\n",
    "                cumul_reward += r\n",
    "                if time_step % 99 == 0:\n",
    "                    with open(\"results.txt\", \"a\") as myfile:\n",
    "                        myfile.write(\"{} | Action: {}, Reward: {}\".format(time_step, a, cumul_reward))\n",
    "                        myfile.write(\"\\n\")\n",
    "                time_step += 1\n",
    "                \n",
    "                if done or (step == (self.steps - 1)):\n",
    "                    with open(\"results.txt\", \"a\") as myfile:\n",
    "                        end = time.time()\n",
    "                        myfile.write(\"Episode took: {} seconds\".format(end-start))\n",
    "                        myfile.write(\"\\n\")\n",
    "                    break;\n",
    "            #self.save_weights('')\n",
    "            print(\"Score: \" + str(cumul_reward))\n",
    "        return results\n",
    "\n",
    "    def save_weights(self, path):\n",
    "        path += '_LR_{}'.format(self.learning_rate)\n",
    "        self.actor.save(path)\n",
    "        self.critic.save(path)\n",
    "\n",
    "    def load_weights(self, path_actor, path_critic):\n",
    "        self.critic.load_weights(path_critic)\n",
    "        self.actor.load_weights(path_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fzi/ids/rottach/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 8 layers into a model with 6 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b0059e752b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m############## Training #################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mddpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_LR_0.001_actor.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_LR_0.001_actor.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-be28960bf6e7>\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, path_actor, path_critic)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_actor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8a4a161aafd4>\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 8 layers into a model with 6 layers."
     ]
    }
   ],
   "source": [
    "############## Training #################\n",
    "ddpg = DDPG((96,96,1), 3, 10)\n",
    "ddpg.load_weights('_LR_0.001_actor.h5', '_LR_0.001_actor.h5')\n",
    "ddpg.train(env, render=True, batch_size=32, nb_episodes=1000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
