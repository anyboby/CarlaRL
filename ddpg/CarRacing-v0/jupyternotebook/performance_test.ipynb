{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomUniform, normal\n",
    "from keras.layers import Dense, Conv1D, Conv2D, Flatten, Input, MaxPool2D, concatenate, merge\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the racing game as environment for the Reinforcement Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Game Setup ######### \n",
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        # Actual model\n",
    "        self.model = self._build_model()\n",
    "        # Target net\n",
    "        self.target_model = self._build_model()\n",
    "        self.adam_optimizer = self.optimizer()\n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        \n",
    "        state = Input((self.state_size))\n",
    "        # Convolutions\n",
    "        x = Conv2D(64, kernel_size=9, activation='relu', input_shape=self.state_size)(state)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(128, kernel_size=9, activation='relu') (x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        # 256 Features with around 1/16 of the initial picture size\n",
    "        x = Conv2D(256, kernel_size=9, activation='relu') (x)\n",
    "        x = MaxPool2D(pool_size=(2, 2)) (x)\n",
    "        \n",
    "        # Connect convolution and dense layers\n",
    "        # 2D -> 1D (Linearization)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        # 3 hidden layers\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        # Creates 512 x 512 weights\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        \n",
    "        # Defining the output for each dimension seperately\n",
    "        # TODO: Using normal initialization to keep initial outputs near zero\n",
    "        steering = Dense(1,activation='tanh',kernel_initializer=RandomUniform())(x)   \n",
    "        acceleration = Dense(1,activation='sigmoid',kernel_initializer=RandomUniform())(x)   \n",
    "        brake = Dense(1,activation='sigmoid',kernel_initializer=RandomUniform())(x) \n",
    "        out = concatenate([steering,acceleration,brake],axis=-1)\n",
    "        \n",
    "        model = Model(input=state,output=out)        \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def predict(self, state):\n",
    "        \"\"\" Prediction of actor network\n",
    "        \"\"\"\n",
    "        action = self.model.predict(np.expand_dims(state, axis=0))\n",
    "        print(action)\n",
    "        return action\n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Prediction of target network\n",
    "        \"\"\"\n",
    "        return self.target_model.predict(inp)\n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer model weights to target model with a factor of Tau\n",
    "        \"\"\"\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1 - self.tau)* target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    def train(self, states, actions, grads):\n",
    "        \"\"\" Actor Training\n",
    "        \"\"\"\n",
    "        self.adam_optimizer([states, grads])\n",
    "    def optimizer(self):\n",
    "        \"\"\" Actor Optimizer\n",
    "        \"\"\"\n",
    "        action_gdts = K.placeholder(shape=(None, self.action_size))\n",
    "        params_grad = tf.gradients(self.model.output, self.model.trainable_weights, -action_gdts)\n",
    "        grads = zip(params_grad, self.model.trainable_weights)\n",
    "        return K.function([self.model.input, action_gdts], [tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)][1:])\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path + '_actor.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        # Actual model\n",
    "        self.model = self._build_model()\n",
    "        self.model.compile(Adam(self.learning_rate), 'mse')\n",
    "        # Target net for stability\n",
    "        self.target_model = self._build_model()        \n",
    "        self.target_model.compile(Adam(self.learning_rate), 'mse')\n",
    "        # Function to compute Q-value gradients (Actor Optimization)\n",
    "        self.action_grads = K.function([self.model.input[0], self.model.input[1]], K.gradients(self.model.output, [self.model.input[1]]))\n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        state = Input((self.state_size))\n",
    "        x = Conv2D(64, kernel_size=9, activation='relu', input_shape=self.state_size)(state)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(128, kernel_size=9, activation='relu')(x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(256, kernel_size=9, activation='relu')(x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        \n",
    "        # Actions\n",
    "        action_shape = (self.action_size,)\n",
    "        action_layer = Input(shape=action_shape)\n",
    "        \n",
    "        # TODO: In the original paper the actions are merged in the second hidden layer\n",
    "        x = concatenate([Flatten()(x), action_layer])\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        out = Dense(1, activation='linear', kernel_initializer=RandomUniform())(x)\n",
    "        return Model([state, action_layer], out)\n",
    "    \n",
    "    def gradients(self, states, actions):\n",
    "        \"\"\" Compute Q-value gradients w.r.t. states and policy-actions\n",
    "        \"\"\"\n",
    "        return self.action_grads([states, actions])\n",
    "    \n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Prediction of target network\n",
    "        \"\"\"\n",
    "        return self.target_model.predict(inp)\n",
    "    # Why does the Critic have no predict function\n",
    "    \n",
    "    def train_on_batch(self, states, actions, critic_target):\n",
    "        \"\"\" Train the critic network on batch of sampled experience\n",
    "            using the keras function train_on_batch\n",
    "        \"\"\"\n",
    "        return self.model.train_on_batch([states, actions], critic_target)\n",
    "    \n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer model weights to target model with a factor of Tau\n",
    "        \"\"\"\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1 - self.tau)* target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path + '_critic.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Utils\n",
    "import numpy\n",
    "\n",
    "\"\"\" Original Code by @jaara: https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "\"\"\"\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Utils\n",
    "from collections import deque\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    \"\"\" Memory Buffer Helper class for Experience Replay\n",
    "    using a double-ended queue or a Sum Tree (for PER)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size, with_per = False):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        if(with_per):\n",
    "            # Prioritized Experience Replay\n",
    "            self.alpha = 0.5\n",
    "            self.epsilon = 0.01\n",
    "            self.buffer = SumTree(buffer_size)\n",
    "        else:\n",
    "            # Standard Buffer\n",
    "            self.buffer = deque()\n",
    "        self.count = 0\n",
    "        self.with_per = with_per\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state, error=None):\n",
    "        \"\"\" Save an experience to memory, optionally with its TD-Error\n",
    "        \"\"\"\n",
    "\n",
    "        experience = (state, action, reward, done, new_state)\n",
    "        if(self.with_per):\n",
    "            priority = self.priority(error[0])\n",
    "            self.buffer.add(priority, experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            # Check if buffer is already full\n",
    "            if self.count < self.buffer_size:\n",
    "                self.buffer.append(experience)\n",
    "                self.count += 1\n",
    "            else:\n",
    "                self.buffer.popleft()\n",
    "                self.buffer.append(experience)\n",
    "\n",
    "    def priority(self, error):\n",
    "        \"\"\" Compute an experience priority, as per Schaul et al.\n",
    "        \"\"\"\n",
    "        return (error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\" Current Buffer Occupation\n",
    "        \"\"\"\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Sample a batch, optionally with (PER)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "\n",
    "        # Sample using prorities\n",
    "        if(self.with_per):\n",
    "            T = self.buffer.total() // batch_size\n",
    "            for i in range(batch_size):\n",
    "                a, b = T * i, T * (i + 1)\n",
    "                s = random.uniform(a, b)\n",
    "                idx, error, data = self.buffer.get(s)\n",
    "                batch.append((*data, idx))\n",
    "            idx = np.array([i[5] for i in batch])\n",
    "        # Sample randomly from Buffer\n",
    "        elif self.count < batch_size:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Return a batch of experience\n",
    "        s_batch = np.array([i[0] for i in batch])\n",
    "        a_batch = np.array([i[1] for i in batch])\n",
    "        r_batch = np.array([i[2] for i in batch])\n",
    "        d_batch = np.array([i[3] for i in batch])\n",
    "        new_s_batch = np.array([i[4] for i in batch])\n",
    "        return s_batch, a_batch, r_batch, d_batch, new_s_batch, idx\n",
    "\n",
    "    def update(self, idx, new_error):\n",
    "        \"\"\" Update priority for idx (PER)\n",
    "        \"\"\"\n",
    "        self.buffer.update(idx, self.priority(new_error))\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\" Clear buffer / Sum Tree\n",
    "        \"\"\"\n",
    "        if(self.with_per): self.buffer = SumTree(buffer_size)\n",
    "        else: self.buffer = deque()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \"\"\" Deep Deterministic Policy Gradient (DDPG) Helper Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, batch_no):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        # Environment and A2C parameters\n",
    "        self.state_size = (batch_no,) + state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.00005\n",
    "        # Create actor and critic networks\n",
    "        self.actor = Actor(state_size, self.action_size, 0.1 * self.learning_rate, 0.001)\n",
    "        self.critic = Critic(state_size, self.action_size, self.learning_rate, 0.001)\n",
    "        self.buffer = MemoryBuffer(20000)\n",
    "\n",
    "    def policy_action(self, s):\n",
    "        \"\"\" Use the actor to predict value\n",
    "        \"\"\"\n",
    "        return self.actor.predict(s)[0]\n",
    "\n",
    "    def bellman(self, rewards, q_values, dones):\n",
    "        \"\"\" Use the Bellman Equation to compute the critic target\n",
    "        \"\"\"\n",
    "        critic_target = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                critic_target[i] = rewards[i]\n",
    "            else:\n",
    "                critic_target[i] = rewards[i] + self.gamma * q_values[i]\n",
    "        return critic_target\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state):\n",
    "        \"\"\" Store experience in memory buffer\n",
    "        \"\"\"\n",
    "        self.buffer.memorize(state, action, reward, done, new_state)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return self.buffer.sample_batch(batch_size)\n",
    "\n",
    "    def update_models(self, states, actions, critic_target):\n",
    "        \"\"\" Update actor and critic networks from sampled experience\n",
    "        \"\"\"\n",
    "        # Train critic\n",
    "        self.critic.train_on_batch(states, actions, critic_target)\n",
    "        # Q-Value Gradients under Current Policy\n",
    "        actions = self.actor.model.predict(states)\n",
    "        grads = self.critic.gradients(states, actions)\n",
    "        # Train actor\n",
    "        self.actor.train(states, actions, np.array(grads).reshape((-1, self.action_size)))\n",
    "        # Transfer weights to target networks at rate Tau\n",
    "        self.actor.transfer_weights()\n",
    "        self.critic.transfer_weights()\n",
    "\n",
    "    def performance(self, env, nb_episodes):\n",
    "        for e in range(nb_episodes):\n",
    "            # Reset episode\n",
    "            time, cumul_reward, done = 0, 0, False\n",
    "            old_state = env.reset()\n",
    "            actions, states, rewards = [], [], []\n",
    "            noise = OrnsteinUhlenbeckProcess(size=self.action_size, theta=.15, mu=0., sigma=.3)\n",
    "\n",
    "            while not done:\n",
    "                env.render()\n",
    "                a = self.policy_action(old_state)\n",
    "                a = np.clip(a+noise.sample(), -1, 1)\n",
    "                new_state, r, done, _ = env.step(a)\n",
    "                cumul_reward += r\n",
    "                time += 1\n",
    "            print(\"Score: \" + str(cumul_reward))\n",
    "        return results\n",
    "\n",
    "    def save_weights(self, path):\n",
    "        path += '_LR_{}'.format(self.learning_rate)\n",
    "        self.actor.save(path)\n",
    "        self.critic.save(path)\n",
    "\n",
    "    def load_weights(self, path_actor, path_critic):\n",
    "        self.critic.load_weights(path_critic)\n",
    "        self.actor.load_weights(path_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /fzi/ids/rottach/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fzi/ids/rottach/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1248..1564 -> 316-tiles track\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n",
      "[[ 0.9949165   0.9728229   0.05939214]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-044c6ef8e082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m '''\n\u001b[1;32m     20\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_LR_5e-05_actor.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_LR_5e-05_critic.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3c3a10e5be7d>\u001b[0m in \u001b[0;36mperformance\u001b[0;34m(self, env, nb_episodes)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3c3a10e5be7d>\u001b[0m in \u001b[0;36mpolicy_action\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \"\"\" Use the actor to predict value\n\u001b[1;32m     20\u001b[0m         \"\"\"\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbellman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e6a0c2a3f581>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\" Prediction of actor network\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############## Training #################\n",
    "\n",
    "ddpg = DDPG(env.observation_space.shape, 3, 10)\n",
    "\n",
    "'''\n",
    "for episode in range(2):\n",
    "    score=0\n",
    "    done=False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = ddpg.policy_action(state)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        ddpg.memorize(observation, action, reward, done)\n",
    "        env.render()\n",
    "        score+=reward\n",
    "        state = observation\n",
    "    ddpg.train()\n",
    "    print(\"episode {} score {}\".format(episode, score))\n",
    "'''\n",
    "ddpg.load_weights('_LR_5e-05_actor.h5', '_LR_5e-05_critic.h5')\n",
    "ddpg.performance(env,nb_episodes=100)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
