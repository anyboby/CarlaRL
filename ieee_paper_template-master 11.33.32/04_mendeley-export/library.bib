Automatically generated by Mendeley Desktop 1.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop


@article{Barth2008,
author = {Barth, Alexander and Franke, Uwe},
doi = {10.1109/IVS.2008.4621210},
file = {:home/kuhnt-local/checkout/floh-bib/pr\"{a}diktion/Barth2008\_Prediction.pdf:pdf},
isbn = {978-1-4244-2568-6},
journal = {2008 IEEE Intelligent Vehicles Symposium},
month = jun,
pages = {1068--1073},
publisher = {Ieee},
title = {{Where will the oncoming vehicle be the next second?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4621210},
year = {2008}
}


@article{dosovitskiy2017carla,
  title={CARLA: An open urban driving simulator},
  author={Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1711.03938},
  year={2017}
}

@misc{dziubinskiSemanticSegmentationSemantic2019,
  title = {From Semantic Segmentation to Semantic Bird's-Eye View in the {{CARLA}} Simulator},
  abstract = {Building a neural network for reconstructing the top-down view from four side cameras.},
  language = {en},
  journal = {Medium},
  howpublished = {https://medium.com/asap-report/from-semantic-segmentation-to-semantic-birds-eye-view-in-the-carla-simulator-1e636741af3f},
  author = {Dziubi{\'n}ski, Maciek},
  month = may,
  year = {2019},
  file = {/home/mo/Zotero/storage/GWG7GG9G/from-semantic-segmentation-to-semantic-birds-eye-view-in-the-carla-simulator-1e636741af3f.html}
}

@incollection{zhangGeneralizedCrossEntropy2018,
  title = {Generalized {{Cross Entropy Loss}} for {{Training Deep Neural Networks}} with {{Noisy Labels}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  author = {Zhang, Zhilu and Sabuncu, Mert},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {8778--8788},
  file = {/home/mo/Zotero/storage/Y8AKAUNG/Zhang and Sabuncu - 2018 - Generalized Cross Entropy Loss for Training Deep N.pdf}
}


@article{mnihAsynchronousMethodsDeep2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.01783},
  primaryClass = {cs},
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  language = {en},
  journal = {arXiv:1602.01783 [cs]},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Machine Learning},
  file = {/home/mo/Zotero/storage/ENSTJ42W/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf}
}


@misc{janischLetMakeA3C,
  title = {Let's Make an {{A3C}}: {{Implementation}}},
  shorttitle = {Let's Make an {{A3C}}},
  abstract = {MACHINE LEARNING \& AI},
  journal = {ヤロミル},
  howpublished = {https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/},
  author = {Janisch, Jarom{\'i}r},
  file = {/home/mo/Zotero/storage/6YD5WE5E/lets-make-an-a3c-implementation.html}
}

@inproceedings{kendall2019learning,
  title={Learning to Drive in a Day},
  author={Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Bewley, Alex and Shah, Amar},
  booktitle={Proceedings of the International Conference on Robotics and Automation ({ICRA})},
  year={2019}
}

@article{44806,
title	= {Mastering the game of Go with deep neural networks and tree search},
author	= {David Silver and Aja Huang and Christopher J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
year	= {2016},
URL	= {http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html},
journal	= {Nature},
pages	= {484--503},
volume	= {529}
}

@article{mnih2013playing,
  added-at = {2017-12-07T19:44:53.000+0100},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/24140efabf4cab720d020e0e68f1451f6/lukasw},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {4140efabf4cab720d020e0e68f1451f6},
  journal = {arXiv preprint arXiv:1312.5602},
  keywords = {atari final},
  timestamp = {2017-12-07T19:44:53.000+0100},
  title = {Playing atari with deep reinforcement learning},
  url = {https://arxiv.org/pdf/1312.5602.pdf},
  year = 2013
}


@article{lillicrapContinuousControlDeep2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.02971},
  primaryClass = {cs, stat},
  title = {Continuous Control with Deep Reinforcement Learning},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  journal = {arXiv:1509.02971 [cs, stat]},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mo/Zotero/storage/D5C8ZWP8/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf;/home/mo/Zotero/storage/GPMIAA9A/1509.html}
}


@article{DBLP:journals/corr/abs-1904-09503,
  author    = {Jianyu Chen and
               Bodi Yuan and
               Masayoshi Tomizuka},
  title     = {Model-free Deep Reinforcement Learning for Urban Autonomous Driving},
  journal   = {CoRR},
  volume    = {abs/1904.09503},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.09503},
  archivePrefix = {arXiv},
  eprint    = {1904.09503},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-09503},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{7339478,
author={D. {González} and J. {Pérez} and V. {Milanés} and F. {Nashashibi}},
journal={IEEE Transactions on Intelligent Transportation Systems},
title={A Review of Motion Planning Techniques for Automated Vehicles},
year={2016},
volume={17},
number={4},
pages={1135-1145},
keywords={collision avoidance;intelligent transportation systems;road safety;sensors;motion planning technique;automated vehicles;intelligent vehicles;scene information;onboard sensors;communication network systems;safety;comfort;energy optimization;dynamic environments;obstacle avoidance capabilities;vulnerable road users;VRU;cooperative maneuvers;Vehicles;Planning;Robots;Lattices;Path planning;Vehicle dynamics;Motion planning;automated vehicles;path planning;intelligent transportation systems;Motion planning;automated vehicles;path planning;intelligent transportation systems},
doi={10.1109/TITS.2015.2498841},
ISSN={},
month={April},
}

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@article{Schaul2015PrioritizedER,
  title={Prioritized Experience Replay},
  author={Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
  journal={CoRR},
  year={2015},
  volume={abs/1511.05952}
}

@inproceedings{7995727,
author={P. {Wolf} and C. {Hubschneider} and M. {Weber} and A. {Bauer} and J. {Härtl} and F. {Dürr} and J. M. {Zöllner}},
booktitle={2017 IEEE Intelligent Vehicles Symposium (IV)},
title={Learning how to drive in a real world simulation with deep Q-Networks},
year={2017},
volume={},
number={},
pages={244-250},
keywords={intelligent transportation systems;learning (artificial intelligence);road vehicles;real world simulation;deep Q-networks;reinforcement learning;vehicle steering;3D physics simulation;camera image input;human driving behavior learning;vehicle agent;Training;Automobiles;Learning (artificial intelligence);Visualization;Cameras;Physics;Computer architecture},
doi={10.1109/IVS.2017.7995727},
ISSN={},
month={June},
}

@inproceedings{Djuric2018ShorttermMP,
  title={Short-term Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks},
  author={Nemanja Djuric and Vladan Radosavljevic and Henggang Cui and Thị Thanh V{\^a}n Nguyễn and Fang-Chieh Chou and T. H. Lin and Jeff G. Schneider},
  year={2018}
}

@article{DBLP:journals/corr/abs-1903-00640,
  author    = {Jianyu Chen and
               Bodi Yuan and
               Masayoshi Tomizuka},
  title     = {Deep Imitation Learning for Autonomous Driving in Generic Urban Scenarios
               with Enhanced Safety},
  journal   = {CoRR},
  volume    = {abs/1903.00640},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.00640},
  archivePrefix = {arXiv},
  eprint    = {1903.00640},
  timestamp = {Sat, 30 Mar 2019 19:27:21 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-00640},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1812-03079,
  author    = {Mayank Bansal and
               Alex Krizhevsky and
               Abhijit S. Ogale},
  title     = {ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing
               the Worst},
  journal   = {CoRR},
  volume    = {abs/1812.03079},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.03079},
  archivePrefix = {arXiv},
  eprint    = {1812.03079},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-03079},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1710-02410,
  author    = {Felipe Codevilla and
               Matthias M{\"{u}}ller and
               Alexey Dosovitskiy and
               Antonio L{\'{o}}pez and
               Vladlen Koltun},
  title     = {End-to-end Driving via Conditional Imitation Learning},
  journal   = {CoRR},
  volume    = {abs/1710.02410},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.02410},
  archivePrefix = {arXiv},
  eprint    = {1710.02410},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-02410},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v37-schulman15,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@article{DBLP:journals/corr/SchulmanWDRK17,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanWDRK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Levine:2013:GPS:3042817.3042937,
 author = {Levine, Sergey and Koltun, Vladlen},
 title = {Guided Policy Search},
 booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
 series = {ICML'13},
 year = {2013},
 location = {Atlanta, GA, USA},
 pages = {III-1--III-9},
 url = {http://dl.acm.org/citation.cfm?id=3042817.3042937},
 acmid = {3042937},
 publisher = {JMLR.org},
} 

@inproceedings{Bruin2015TheIO,
  title={The importance of experience replay database composition in deep reinforcement learning},
  author={Tim de Bruin and Jens Kober and Karl Tuyls},
  year={2015}
}

@misc{lilianweng,
  title = {Policy Gradient Algorithms},
  howpublished = {\url{https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#ppo}},
  note = {Accessed: 2019-10-20}
}

