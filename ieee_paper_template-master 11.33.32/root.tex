%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed

%%%%%%%%%% my includes %%%

\usepackage{url}      % needed for ieeetran bib style
\usepackage{graphicx}
\graphicspath{{./03_graphics/}}

% possible subfigure packages
%\usepackage{subfigure}
%\usepackage[caption=false,font=footnotesize]{subfig}

\usepackage[colorinlistoftodos, german]{todonotes} % Option 'disable' entfernt alle ToDos

\usepackage[utf8]{inputenc}

\usepackage[font=footnotesize]{caption}
\usepackage[font=footnotesize]{subcaption}
\newtheorem{thm}{Theorem}[section]
\newtheorem{defn}[thm]{Definition}


\usepackage{hyperref}

%% format tabular cells
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{makecell}
\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

%\usepackage[style=plain,citestyle=numeric,bibstyle=numeric,sorting=none,url=false,doi=false,isbn=false]{biblatex}

%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{\LARGE \bf
Deep Reinforcement Learning with Continuous Control in CARLA
}

\author{Moritz Zanger$^{1}$Florian Rottach$^{1}$Izel Kilinic$^{1}$ % <-this % stops a space
\thanks{$^{1}$The authors are with FZI Research Center for Information Technology, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany
%        {\tt\small \{kuhnt, zoellner\}@fzi.de}}%
        {\tt\small \{kuhnt, zoellner\}@fzi.de}}%
}        
        
\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

%This electronic document is a ÒliveÓ template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.

Style:
\begin{itemize}
	\item Written last
	\item 100-250 words
	\item Past tense
\end{itemize}

Content:
\begin{itemize}
	\item Condensed version of entire article
\end{itemize}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

%This template provides authors with most of the formatting specifications needed for preparing electronic versions of their papers. All standard paper components have been specified for three reasons: (1) ease of use when formatting individual papers, (2) automatic compliance to electronic requirements that facilitate the concurrent or later production of electronic products, and (3) conformity of style throughout a conference proceedings. Margins, column widths, line spacing, and type styles are built-in; examples of the type styles are provided throughout this document and are identified in italic type, within parentheses, following the example. Some components, such as multi-leveled equations, graphics, and tables are not prescribed, although the various table text styles are provided. The formatter will need to create these components, incorporating the applicable criteria that follow.

%Example Citation: \cite{Barth2008}.

The task of teaching vehicles how to drive autonomously in urban scenarios is a challenging and complex one to solve. Not only is there the problem of finding the adequate response to a given situation but also the challenge of taking into account the surrounding factors that have an influence on the state that a vehicle is in and its possible actions. To date, most approaches focus on the manual design of behavioral policies, such as defining a driving policy through the use of annotated maps. While these solutions might work in situations which are documented by the provided mapping infrastructure, they are often difficult to generalize or scale, as they do not necessarily enable the comprehension of any given local scene. In order to make autonomous driving truly feasible in a real-world scenario it would be better to develop systems which are able to find their way without having to rely on an explicit set of rules. One possible solution to this task is provided by reinforcement learning methods. Here, the agent, i.e. the vehicle, actively searches for the optimal driving policy whilst trying to maximize a numerical reward signal. As opposed to imitation learning techniques, which have been popular in finding driving policies (1), reinforment learning algorithms enable a car to exceed human abilities, if applied correctly. In recent years, deep reinforcement learning methods have proven to be succesful in solving complex tasks such as playing GO (2) or Atari (3) and there have been efforts in tackling various problems in the field of autonomous driving, including continous control tasks (4).

However, two major drawbacks of reinforcement learning methods are their heavy depency on adequate input state representations (5) and, as with other machine learning techniques, their need of a sufficient amount of accurate sample data to train on. In order to be able to train safely on an adequate amount of data, one approach is the use of data from other domains. For this purpose, the urban driving simulator CARLA has been developed, which is used as a simulation environment for this project.

In this paper, several state-of-the-art reinforcement learning algorithms are implemented and compared, with regard to their performance considering different driving tasks. Additionally, reward functions for the respective problems are tested and several input represantations are designed and evaluated.

?? What is it exactly that we contributed that is new to already existing research??

%\subsection{Problem specification}
%\subsection{Why RL? What is our goal/motivation?}

\section{Related Work}

%“Human-level control through deep reinforcement learning” (2015)
%“CARLA: An Open Urban Driving Simulator”
%Our contribution to the field

\section{Background}

We regard the typical reinforcement learning setting where an agent interacts with an environment $\mathcal{E}$. At each one of a number of discrete timesteps $t$ the agent decides on taking an action $a_t$ from a given set of actions $\mathcal{A}$. This is done based on the state $s_t$ that the agent is currently in and following a policy $\pi$, which is a mapping of the possible states to the action space $\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$. In this case, $\pi$ is stochastic, as it returns the probability distribution over the possible actions, and the action space is continuous.

As a result, the agent receives a reward $r_t$ and the subsequent state $s_{t+1}$ of his environment. The setup is assumed to follow the properties of a Markov decision process, where besides state space $\mathcal{S}$, action space $\mathcal{A}$ and reward function $r(s_t,a_t)$, we also include the transition function to the future states $p(s_{t+1}|s_t,a_t)$.

\section{Concept/Methods and Models}

In the course of this project, three differnt algorithms are used. Following the work of Lillicrap et al. (4) and Mnih et al. (6), the (DDPG)- and (A3C) algorithms are implemented and compared according to their performance in the Gym environment CarRacing-v0. Later on, the PPO algorithm is applied to solve a continuous control task in the CARLA simulator.
TODO: Hier unser Konzept analog zur Präsentation vorstellen (Slide mit den 4 Kasten)
%\subsection{CarRacing - Understand and select algorithms}
%\subsubsection{Preprocessing}
%\subsubsection{DQN}

\subsection{DDPG}
One very notable advance in reinforcement learning has been made by the development of the so-called "Deep Q Network" (Mnih et al., 2015). The DQN is able to solve tasks with high-dimensional observation spaces. However, it is only efficiently capable of working with discrete and low-dimensional action spaces. In order to adapt a (DQN) for the successful use with continuous control problems, as given in CarRacing-v0, a discretization of the action space has to be carried out, which can lead to two main difficulties: an explosion in the number of possible actions and the loss of important information (4).

To evade these obstacles (Lillicrap et al.) propose a new approach, called the Deep Deterministic Policy Gradient (DDPG), which is a model-free, off-policy actor-critic algorithm. They adopt the advantages of (DQN) and combine them with the actor-critic framework, resulting in the stabilization of Q-learning by using a replay buffer and soft updates on the target networks of both actor and critic, through

\begin{equation}
\tau << 1:\theta' \leftarrow \tau\theta + (1-\tau)\theta'
\end{equation}

and finding a deterministic policy.

Noise with Ornstein-Uhlenbeck

\subsection{A3C}
The Asynchronous Advantage Actor-Critic (A3C), introduced by Mnih et al. \cite{mnihAsynchronousMethodsDeep2016}, has become a go-to algorithm in 
Deep Reinforcement Learning due to its performance, robustness, and ability to perform well on high-dimensional action- and state-spaces. 
A key characteristic of A3C is the utilization of multiple agents, each equipped with its own environment instance and its own set of network parameters. 
One of the advantages of this approach is the diversification of the collected experience but yields the challenge of handling gradient update 
mismatches between the asynchronously collected network parameter updates. 
In our implementation of A3C we adapted several alterations as opposed to the original version by Mnih et al.. Despite being an on-policy method, we decided 
to include a numerically efficient n-step return as proposed by jaromiru \cite{janischLetMakeA3C}. % (eq. x). 
% \begin{equation}
%    R_{0} = r_{0} + \gamma r_1 + \gamma^2 r_2 + ... + \gamma^{n-1} r_{n-1}
%    R_{1} = r_{1} + \gamma r_2 + \gamma^2 r_3 + ... + \gamma^{n-1} r_{n}
% \end{equation}
% With R denoting the discounted Return at each timestep, $\gamma$ the discount factor, and r the reward at each timestep. Rather than calculating 

Furthermore, experiences are buffered in a global update 
queue and updates are only performed by a master network. This feature helps decorrelating experiences, emphasizes exploration and allows for more gpu-friendly
batch-learning. However, one has to keep in mind that this contradicts the original update rule by mnih et al. and might lead to policy lag. A proper update 
frequency in the master network is therefore of major importance.

\subsection{PPO}
\subsection{Reward function}
The design of the reward function turned out to be a much more difficult procedure as expected initially. The already provided rewards in gym's CarRacing environment were rather simple and still led to a good result in the end. For CARLA however, it was necessary to invest more time into the engineering and designing of a suitable inducement system, because the driving behavior depended on more factors and the performance metric was not just driving as fast as possible. In the following, the process of arriving at our final reward function will be described. \newline 
In the first step, we investigated possible sensor inputs that might have an impact on the driving behavior and discussed on their impact. The outcome is summarized in the following table:
\newline 

\begin{table}
\footnotesize
\centering
\caption{Moritz' krankes table}%
\label{tab:Example}%
\begin{tabularx}{\linewidth}{lcX}%
\toprule
\textbf{Component} & \textbf{Intention} & \textbf{Weight} \\
\midrule
\makecell[Xt]{Per frame penalty}          & \makecell[Xt]{Forces agent to move}  &\makecell[lt]{-0.1} \\
\makecell[Xt]{Lane invasion counter}          & \makecell[Xt]{As few lane changes as possible}  &\makecell[lt]{-0.1} \\
\makecell[Xt]{Steering angle}          & \makecell[Xt]{Avoids oszillations}  &\makecell[lt]{-0.1} \\
\makecell[Xt]{Delta heading}          & \makecell[Xt]{Drive as straight as possible relative to road}  &\makecell[lt]{-0.1} \\
\bottomrule
\end{tabularx}
\end{table}
   
   



TODO: Schön formatieren
\begin{tabular}{ | l | l | l | p{5cm} |}
\hline

\hline

\hline
Per frame penalty & Forces agent to move \\
\hline
Lane invasion counter & As few lane changes as possible \\
\hline
Steering angle change & Avoids oszillations \\
\hline
Delta heading  & Drive as straight as possible relative to road \\
\hline
Position change & Maximize travelled distance \\
\hline
Collision binary per frame & Avoid crashes  \\
\hline
Velocity & Drive fast under the other constraints\\
\hline
Distance to middle lane & Drive as centered as possible\\
\hline
\end{tabular}
\newline

We started with incrementally adding these attributes to our reward calculation and quickly realized, that the main challenge is to adjust the weights and harmonize the contrary effects of the terms.
An example would be, that giving the velocity a relatively high weight, such as 0.8, while giving the distance to center line a weight of 0.2 results in an agent that speeds over the map and pays few attention on lane invasions. On the opposite side, the agent will drive only very slowly or even not at all, if the rewards for oscillations are too high compared to the velocity. Considering, that we have not only two, but rather several possible components, this results in a complex combinatorical problem that can either be solved by trail and error or applying permutation optimization techniques. To achieve initial results, we started to discover a proper reward function "by hand". 
\newline
We pruned the above list by applying the following considerations that resulted from tesing different approaches. Firstly, some components show a redundant behavior and hence one of them can be removed. An example would be the velocity and the position change - both contain the same information. Further, the attributes lane invasion and steering angle didnn't affect the driving behavior in a positive way. The two most important parameters turned out be the velocity and the delta heading, which expressed the relative angle to the current street angle. Our final reward function and the aggregation weights can be found in the following table:
\newline
TODO: Werte aus Präsentation übernehmen
\begin{tabular}{ | l | l | l | p{5cm} |}
	\hline
	hier & drei & spalten
\end{tabular}
\newline

The result is very promising and can be summarized as follows: The agent is capable of driving smoothly within the right lane and can perform turn manuevers on most intersections. It attempts to drive around other vehicles, but is not capable of breaking. We trained different models on this reward function and all of them had a good performance compared to other functions.


\subsection{Input representation}
Dealing with the high-dimensional environment in CARLA is one of the central challenges in implementing a well functioning 
RL algorithm. In consideration of an abundance of possible sensor types available in CARLA we decided to pursue increasing
levels of realism which generally also correspond to increasing levels of difficulty. These aforementioned levels of realism involve the following:
\begin{itemize}
    \item Ground truth segmented bird's eye view
    \item Ground truth segmented front view
    \item Latent space generated from ground truth segmented images
    \item Latent space generated from rgb images
\end{itemize}
Notably, these input representations differ in dimensions and thus lead to different network architectures in the appended network architectures 
of the RL agent. 
\paragraph{Ground truth segmented bird's eye view}
The Ground truth segmented bird's eye view is a rather unrealistic scenario, in which we assume availability of a camera positioned 
20m above the performing agent. It is merely imaginable in a fully observed city where autonomous driving is part of a high-level traffic 
control system. Albeit rather unrealistic, we decided to implement a 13-class ground truth segmented bird's eye view due to its similarity to 
the CarRacing 
environment and its obvious advantages in containing information central to navigational tasks. The full list of the 
included classes are described by dosovitskiy et al.\cite{dosovitskiy2017carla}. We deemed a 1-channel 
grayscale image with size 64x64 
large enough to contain key information for the algorithm (fig. x l.)

\begin{figure}[thpb]
    \centering
    \framebox{\parbox{3in}{
    \includegraphics[scale=.271]{gt_combined.png}
    }}
    \caption{left: Ground truth segmented bird's eye view image, 64x64 in grayscale, 9 classes
    right: Ground truth segmented front view image, 80x80 in grayscale, 9 classes}
        \label{figurelabel}
        \end{figure}

\paragraph{Ground truth segmented frontview}
Much like the Ground truth segmented bird's eye view, the corresponding front view input representation assumes the availability of a 
perfect segmentation camera. Nonetheless, we increase realism with this representation as the camera is placed in front of the car.
Again, we chose a 13-class, 1-channel grayscale image with a slightly increased size of 80x80 to account for a larger view angle of the 
camera. 
\paragraph{Latent space generated from ground truth segmented images}
In this section we will further discuss a modified version of the integrated encoder-decoder network that is based on 
dziubinski's \cite{dziubinskiSemanticSegmentationSemantic2019} implementation. 
The underlying idea of this model is to 
guide feature extraction towards more useful, problem-specific features by exposing the model to the additional 
target of reconstructing a bird's eye view from the vehicle-based camera images.
In its original architecture, the network comprises 
5 input tensors and 7 output tensors with the inputs representing images of a front-, rear-, left-, right, and top-view camera. The 
model is built with the Keras functional API and generally serves two purposes. On the one hand, each input tensor is encoded into 
a 64-sized feature vector and decoded into it's original shape with a cross entropy loss with regard to the original input. This is 
achieved with separate branches of autoencoders with 3 convolutions respectively. On the other 
hand, a separate generative branch creates a bird's eye view reconstruction based on the concatenated feature vectors of the 
vehicle-based cameras. In addition to the cross entropy loss at the reconstructed bird's eye view output, a mean square error 
loss is calculated against the output of a subtract layer between the autoencoder's feature vector und the reconstructed feature vector to support
convergence. Notably, the autoencoder branch of the top-down camera view is solely purposed for improving the latent space of the generative branch 
during training and is thus not required for inference. 
\newline In comparison to dziubinski's \cite{dziubinskiSemanticSegmentationSemantic2019} vanilla version, we 
adjusted the architecture to fit the single camera bottlenecks into a length 64 vector and the reconstructed bird's eye view
bottleneck into a length 128 vector. To reduce complexity, we condensed both input and target segmentation images to 3 classes 
and implemented a generalized weighted cross entropy\cite{zhangGeneralizedCrossEntropy2018} loss function to 
account for the unbalanced distribution of vehicles and obstacles.
\begin{figure}[thpb]
   \centering
   \framebox{\parbox{3in}{
   \includegraphics[scale=.113]{rgb_birdseye.png}
   }}
   \caption{left: rgb images from 4 vehicle-based cameras
            \newline 2nd from right: ground truth segmented bird's eye view
            \newline right: reconstructed bird's eye view}
       \label{figurelabel} 
       \end{figure}

\paragraph{Latent space generated from rgb images} To deal with the higher input complexity in RGB-images, we extended the
encoder and decoder models to 5 convolutions.
       
\subsection{Autoencoder/Latent space}
TODO: Encoder-decoder architecture bzw. Generator nennen
\subsection{Training}
TODO: Training beschreiben analog zur präsentation (vgl folien)

%\subsection{CARLA}

\section{Evaluation}

\subsection{CarRacing-v0}
\subsection{CARLA}

\subsection{Results}
\subsection{Comparison algorithms/reward functions}

\section{Conclusions}
--- mo 
Despite the weighted categorical cross entropy loss, the network struggled with
reconstructing pixels of the \textit{vehicle and obstacles} class. 
Aside from improving training data and network parameters, we 
consider a more sophisticated encoding model as crucial for better results. 
--- mo
\begin{itemize}
      \item Similar to the abstract but more detail
      \item Conclusion of the key points of each section
      \item Summary of main findings
      \item Important conclusions that can be drawn
      \item Discuss benefits and shortcomings of our approach
      \item Suggest future areas of research
   \end{itemize}


\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.










\newpage

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,04_mendeley-export/library}


\newpage

\appendices



\end{document}
