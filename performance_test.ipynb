{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomUniform, normal\n",
    "from keras.layers import Dense, Conv1D, Conv2D, Flatten, Input, MaxPool2D, concatenate, merge\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the racing game as environment for the Reinforcement Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Game Setup ######### \n",
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        # Actual model\n",
    "        self.model = self._build_model()\n",
    "        # Target net\n",
    "        self.target_model = self._build_model()\n",
    "        self.adam_optimizer = self.optimizer()\n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        \n",
    "        state = Input((self.state_size))\n",
    "        # Convolutions\n",
    "        x = Conv2D(64, kernel_size=9, activation='relu', input_shape=self.state_size)(state)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(128, kernel_size=9, activation='relu') (x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        # 256 Features with around 1/16 of the initial picture size\n",
    "        x = Conv2D(256, kernel_size=9, activation='relu') (x)\n",
    "        x = MaxPool2D(pool_size=(2, 2)) (x)\n",
    "        \n",
    "        # Connect convolution and dense layers\n",
    "        # 2D -> 1D (Linearization)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        # 3 hidden layers\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        # Creates 512 x 512 weights\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        \n",
    "        # Defining the output for each dimension seperately\n",
    "        # TODO: Using normal initialization to keep initial outputs near zero\n",
    "        steering = Dense(1,activation='tanh',kernel_initializer=RandomUniform())(x)   \n",
    "        acceleration = Dense(1,activation='sigmoid',kernel_initializer=RandomUniform())(x)   \n",
    "        brake = Dense(1,activation='sigmoid',kernel_initializer=RandomUniform())(x) \n",
    "        out = concatenate([steering,acceleration,brake],axis=-1)\n",
    "        \n",
    "        model = Model(input=state,output=out)        \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def predict(self, state):\n",
    "        \"\"\" Prediction of actor network\n",
    "        \"\"\"\n",
    "        action = self.model.predict(np.expand_dims(state, axis=0))\n",
    "        print(action)\n",
    "        return action\n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Prediction of target network\n",
    "        \"\"\"\n",
    "        return self.target_model.predict(inp)\n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer model weights to target model with a factor of Tau\n",
    "        \"\"\"\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1 - self.tau)* target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    def train(self, states, actions, grads):\n",
    "        \"\"\" Actor Training\n",
    "        \"\"\"\n",
    "        self.adam_optimizer([states, grads])\n",
    "    def optimizer(self):\n",
    "        \"\"\" Actor Optimizer\n",
    "        \"\"\"\n",
    "        action_gdts = K.placeholder(shape=(None, self.action_size))\n",
    "        params_grad = tf.gradients(self.model.output, self.model.trainable_weights, -action_gdts)\n",
    "        grads = zip(params_grad, self.model.trainable_weights)\n",
    "        return K.function([self.model.input, action_gdts], [tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)][1:])\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path + '_actor.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        # Actual model\n",
    "        self.model = self._build_model()\n",
    "        self.model.compile(Adam(self.learning_rate), 'mse')\n",
    "        # Target net for stability\n",
    "        self.target_model = self._build_model()        \n",
    "        self.target_model.compile(Adam(self.learning_rate), 'mse')\n",
    "        # Function to compute Q-value gradients (Actor Optimization)\n",
    "        self.action_grads = K.function([self.model.input[0], self.model.input[1]], K.gradients(self.model.output, [self.model.input[1]]))\n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        state = Input((self.state_size))\n",
    "        x = Conv2D(64, kernel_size=9, activation='relu', input_shape=self.state_size)(state)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(128, kernel_size=9, activation='relu')(x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(256, kernel_size=9, activation='relu')(x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        \n",
    "        # Actions\n",
    "        action_shape = (self.action_size,)\n",
    "        action_layer = Input(shape=action_shape)\n",
    "        \n",
    "        # TODO: In the original paper the actions are merged in the second hidden layer\n",
    "        x = concatenate([Flatten()(x), action_layer])\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        out = Dense(1, activation='linear', kernel_initializer=RandomUniform())(x)\n",
    "        return Model([state, action_layer], out)\n",
    "    \n",
    "    def gradients(self, states, actions):\n",
    "        \"\"\" Compute Q-value gradients w.r.t. states and policy-actions\n",
    "        \"\"\"\n",
    "        return self.action_grads([states, actions])\n",
    "    \n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Prediction of target network\n",
    "        \"\"\"\n",
    "        return self.target_model.predict(inp)\n",
    "    # Why does the Critic have no predict function\n",
    "    \n",
    "    def train_on_batch(self, states, actions, critic_target):\n",
    "        \"\"\" Train the critic network on batch of sampled experience\n",
    "            using the keras function train_on_batch\n",
    "        \"\"\"\n",
    "        return self.model.train_on_batch([states, actions], critic_target)\n",
    "    \n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer model weights to target model with a factor of Tau\n",
    "        \"\"\"\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1 - self.tau)* target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path + '_critic.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Utils\n",
    "import numpy\n",
    "\n",
    "\"\"\" Original Code by @jaara: https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "\"\"\"\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Utils\n",
    "from collections import deque\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    \"\"\" Memory Buffer Helper class for Experience Replay\n",
    "    using a double-ended queue or a Sum Tree (for PER)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size, with_per = False):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        if(with_per):\n",
    "            # Prioritized Experience Replay\n",
    "            self.alpha = 0.5\n",
    "            self.epsilon = 0.01\n",
    "            self.buffer = SumTree(buffer_size)\n",
    "        else:\n",
    "            # Standard Buffer\n",
    "            self.buffer = deque()\n",
    "        self.count = 0\n",
    "        self.with_per = with_per\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state, error=None):\n",
    "        \"\"\" Save an experience to memory, optionally with its TD-Error\n",
    "        \"\"\"\n",
    "\n",
    "        experience = (state, action, reward, done, new_state)\n",
    "        if(self.with_per):\n",
    "            priority = self.priority(error[0])\n",
    "            self.buffer.add(priority, experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            # Check if buffer is already full\n",
    "            if self.count < self.buffer_size:\n",
    "                self.buffer.append(experience)\n",
    "                self.count += 1\n",
    "            else:\n",
    "                self.buffer.popleft()\n",
    "                self.buffer.append(experience)\n",
    "\n",
    "    def priority(self, error):\n",
    "        \"\"\" Compute an experience priority, as per Schaul et al.\n",
    "        \"\"\"\n",
    "        return (error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\" Current Buffer Occupation\n",
    "        \"\"\"\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Sample a batch, optionally with (PER)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "\n",
    "        # Sample using prorities\n",
    "        if(self.with_per):\n",
    "            T = self.buffer.total() // batch_size\n",
    "            for i in range(batch_size):\n",
    "                a, b = T * i, T * (i + 1)\n",
    "                s = random.uniform(a, b)\n",
    "                idx, error, data = self.buffer.get(s)\n",
    "                batch.append((*data, idx))\n",
    "            idx = np.array([i[5] for i in batch])\n",
    "        # Sample randomly from Buffer\n",
    "        elif self.count < batch_size:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Return a batch of experience\n",
    "        s_batch = np.array([i[0] for i in batch])\n",
    "        a_batch = np.array([i[1] for i in batch])\n",
    "        r_batch = np.array([i[2] for i in batch])\n",
    "        d_batch = np.array([i[3] for i in batch])\n",
    "        new_s_batch = np.array([i[4] for i in batch])\n",
    "        return s_batch, a_batch, r_batch, d_batch, new_s_batch, idx\n",
    "\n",
    "    def update(self, idx, new_error):\n",
    "        \"\"\" Update priority for idx (PER)\n",
    "        \"\"\"\n",
    "        self.buffer.update(idx, self.priority(new_error))\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\" Clear buffer / Sum Tree\n",
    "        \"\"\"\n",
    "        if(self.with_per): self.buffer = SumTree(buffer_size)\n",
    "        else: self.buffer = deque()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \"\"\" Deep Deterministic Policy Gradient (DDPG) Helper Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, batch_no):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        # Environment and A2C parameters\n",
    "        self.state_size = (batch_no,) + state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.00005\n",
    "        # Create actor and critic networks\n",
    "        self.actor = Actor(state_size, self.action_size, 0.1 * self.learning_rate, 0.001)\n",
    "        self.critic = Critic(state_size, self.action_size, self.learning_rate, 0.001)\n",
    "        self.buffer = MemoryBuffer(20000)\n",
    "\n",
    "    def policy_action(self, s):\n",
    "        \"\"\" Use the actor to predict value\n",
    "        \"\"\"\n",
    "        return self.actor.predict(s)[0]\n",
    "\n",
    "    def bellman(self, rewards, q_values, dones):\n",
    "        \"\"\" Use the Bellman Equation to compute the critic target\n",
    "        \"\"\"\n",
    "        critic_target = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                critic_target[i] = rewards[i]\n",
    "            else:\n",
    "                critic_target[i] = rewards[i] + self.gamma * q_values[i]\n",
    "        return critic_target\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state):\n",
    "        \"\"\" Store experience in memory buffer\n",
    "        \"\"\"\n",
    "        self.buffer.memorize(state, action, reward, done, new_state)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return self.buffer.sample_batch(batch_size)\n",
    "\n",
    "    def update_models(self, states, actions, critic_target):\n",
    "        \"\"\" Update actor and critic networks from sampled experience\n",
    "        \"\"\"\n",
    "        # Train critic\n",
    "        self.critic.train_on_batch(states, actions, critic_target)\n",
    "        # Q-Value Gradients under Current Policy\n",
    "        actions = self.actor.model.predict(states)\n",
    "        grads = self.critic.gradients(states, actions)\n",
    "        # Train actor\n",
    "        self.actor.train(states, actions, np.array(grads).reshape((-1, self.action_size)))\n",
    "        # Transfer weights to target networks at rate Tau\n",
    "        self.actor.transfer_weights()\n",
    "        self.critic.transfer_weights()\n",
    "\n",
    "    def performance(self, env, nb_episodes):\n",
    "        for e in range(nb_episodes):\n",
    "            # Reset episode\n",
    "            time, cumul_reward, done = 0, 0, False\n",
    "            old_state = env.reset()\n",
    "            actions, states, rewards = [], [], []\n",
    "            noise = OrnsteinUhlenbeckProcess(size=self.action_size, theta=.15, mu=0., sigma=.3)\n",
    "\n",
    "            while not done:\n",
    "                env.render()\n",
    "                a = self.policy_action(old_state)\n",
    "                a = np.clip(a+noise.sample(), -1, 1)\n",
    "                new_state, r, done, _ = env.step(a)\n",
    "                cumul_reward += r\n",
    "                time += 1\n",
    "            print(\"Score: \" + str(cumul_reward))\n",
    "        return results\n",
    "\n",
    "    def save_weights(self, path):\n",
    "        path += '_LR_{}'.format(self.learning_rate)\n",
    "        self.actor.save(path)\n",
    "        self.critic.save(path)\n",
    "\n",
    "    def load_weights(self, path_actor, path_critic):\n",
    "        self.critic.load_weights(path_critic)\n",
    "        self.actor.load_weights(path_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Training #################\n",
    "\n",
    "ddpg = DDPG(env.observation_space.shape, 3, 10)\n",
    "\n",
    "'''\n",
    "for episode in range(2):\n",
    "    score=0\n",
    "    done=False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = ddpg.policy_action(state)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        ddpg.memorize(observation, action, reward, done)\n",
    "        env.render()\n",
    "        score+=reward\n",
    "        state = observation\n",
    "    ddpg.train()\n",
    "    print(\"episode {} score {}\".format(episode, score))\n",
    "'''\n",
    "ddpg.load_weights('_LR_5e-05_actor.h5', '_LR_5e-05_critic.h5')\n",
    "ddpg.performance(env,nb_episodes=100)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
