{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.layers import Dense, Conv1D, Conv2D, Flatten, Input, MaxPool2D, concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the racing game as environment for the Reinforcement Learning Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Game Setup ######### \n",
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code displays:\n",
    "- states: 96 pixles x 96 pixles x RGB\n",
    "- actions (3 possibilities):\n",
    "\n",
    "[1, 0, 0] = Right % (steering)\n",
    "\n",
    "[-1, 0, 0] = Left % (steering)\n",
    "\n",
    "[0, 1, 0] = Straight % speed\n",
    "\n",
    "[0, 0, 1] = Brake % speed (calculated against speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of params affecting the environment: (96, 96, 3)\n",
      "No of possible actions: Box(3,)\n",
      "Example for random action [0.6665992  0.19215995 0.99388593]\n",
      "GPUs:  []\n"
     ]
    }
   ],
   "source": [
    "########### Game and Environment info ###########\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space\n",
    "random_action = env.action_space.sample()\n",
    "print('No of params affecting the environment:', states)\n",
    "print('No of possible actions:', actions)\n",
    "print('Example for random action', random_action)\n",
    "\n",
    "\n",
    "########### Check if Keras uses GPU ##########\n",
    "from tensorflow.python.client import device_lib\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "#print(device_lib.list_local_devices())\n",
    "print(\"GPUs: \", K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        # Actual model\n",
    "        self.model = self._build_model()\n",
    "        # Target net\n",
    "        self.target_model = self._build_model()\n",
    "        self.adam_optimizer = self.optimizer()\n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        # Build the model layer by layer\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Convolutions\n",
    "        model.add(Conv2D(64, kernel_size=9, activation='relu', input_shape=self.state_size))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(128, kernel_size=9, activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        # Output an der stelle sind 256 features mit jeweils 1/16 der ursprünglichen Bildgröße\n",
    "        model.add(Conv2D(256, kernel_size=9, activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Connect convolution and dense layers\n",
    "        # 2D -> 1D (Linearization)\n",
    "        model.add(Flatten())\n",
    "    \n",
    "        # 3 hidden layers\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        # Creates 512 x 512 weights\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        \n",
    "        # Output neurons (number of actions) (512 x 3)\n",
    "        model.add(Dense(self.action_size, activation='sigmoid'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def predict(self, state):\n",
    "        \"\"\" Prediction of actor network\n",
    "        \"\"\"\n",
    "        # TODO: dimensionen anpassen ggf\n",
    "        action = self.model.predict(np.expand_dims(state, axis=0))\n",
    "        action[0] = (action[0] * 2) - 1; \n",
    "        print(action)\n",
    "        return action\n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Prediction of target network\n",
    "        \"\"\"\n",
    "        return self.target_model.predict(inp)\n",
    "    \n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer model weights to target model with a factor of Tau\n",
    "        \"\"\"\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1 - self.tau)* target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    def train(self, states, actions, grads):\n",
    "        \"\"\" Actor Training\n",
    "        \"\"\"\n",
    "        self.adam_optimizer([states, grads])\n",
    "    def optimizer(self):\n",
    "        \"\"\" Actor Optimizer\n",
    "        \"\"\"\n",
    "        action_gdts = K.placeholder(shape=(None, self.action_size))\n",
    "        params_grad = tf.gradients(self.model.output, self.model.trainable_weights, -action_gdts)\n",
    "        grads = zip(params_grad, self.model.trainable_weights)\n",
    "        return K.function([self.model.input, action_gdts], [tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)][1:])\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path + '_actor.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)\n",
    "    \n",
    "#actor = Actor(env.observation_space.shape, 3, 0.001, 0.1)\n",
    "#actor.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        # Actual model\n",
    "        self.model = self._build_model()\n",
    "        # Target net\n",
    "        self.target_model = self._build_model()        \n",
    "        self.model.compile(Adam(self.learning_rate), 'mse')\n",
    "        self.target_model.compile(Adam(self.learning_rate), 'mse')\n",
    "        # Function to compute Q-value gradients (Actor Optimization)\n",
    "        self.action_grads = K.function([self.model.input[0], self.model.input[1]], K.gradients(self.model.output, [self.model.input[1]]))\n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        # Build the model layer by layer\n",
    "        state = Input((self.state_size))\n",
    "        x = Conv2D(64, kernel_size=9, activation='relu', input_shape=self.state_size)(state)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(128, kernel_size=9, activation='relu')(x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Conv2D(256, kernel_size=9, activation='relu')(x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        \n",
    "        # Actions\n",
    "        action_shape = (self.action_size,)\n",
    "        action_layer = Input(shape=action_shape)\n",
    "        x = concatenate([Flatten()(x), action_layer])\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        out = Dense(1, activation='linear', kernel_initializer=RandomUniform())(x)\n",
    "        return Model([state, action_layer], out)\n",
    "    \n",
    "    def gradients(self, states, actions):\n",
    "        \"\"\" Compute Q-value gradients w.r.t. states and policy-actions\n",
    "        \"\"\"\n",
    "        return self.action_grads([states, actions])\n",
    "    \n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Prediction of target network\n",
    "        \"\"\"\n",
    "        return self.target_model.predict(inp)\n",
    "    # Why does the Critic have no predict function\n",
    "    \n",
    "    def train_on_batch(self, states, actions, critic_target):\n",
    "        \"\"\" Train the critic network on batch of sampled experience\n",
    "            using the keras function train_on_batch\n",
    "        \"\"\"\n",
    "        return self.model.train_on_batch([states, actions], critic_target)\n",
    "    \n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer model weights to target model with a factor of Tau\n",
    "        \"\"\"\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1 - self.tau)* target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path + '_critic.h5')\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path)\n",
    "         \n",
    "#critic = Critic(env.observation_space.shape, 3, 0.001, 0.1)\n",
    "#critic.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Utils\n",
    "import numpy\n",
    "\n",
    "\"\"\" Original Code by @jaara: https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "\"\"\"\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Utils\n",
    "from collections import deque\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    \"\"\" Memory Buffer Helper class for Experience Replay\n",
    "    using a double-ended queue or a Sum Tree (for PER)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size, with_per = False):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        if(with_per):\n",
    "            # Prioritized Experience Replay\n",
    "            self.alpha = 0.5\n",
    "            self.epsilon = 0.01\n",
    "            self.buffer = SumTree(buffer_size)\n",
    "        else:\n",
    "            # Standard Buffer\n",
    "            self.buffer = deque()\n",
    "        self.count = 0\n",
    "        self.with_per = with_per\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state, error=None):\n",
    "        \"\"\" Save an experience to memory, optionally with its TD-Error\n",
    "        \"\"\"\n",
    "\n",
    "        experience = (state, action, reward, done, new_state)\n",
    "        if(self.with_per):\n",
    "            priority = self.priority(error[0])\n",
    "            self.buffer.add(priority, experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            # Check if buffer is already full\n",
    "            if self.count < self.buffer_size:\n",
    "                self.buffer.append(experience)\n",
    "                self.count += 1\n",
    "            else:\n",
    "                self.buffer.popleft()\n",
    "                self.buffer.append(experience)\n",
    "\n",
    "    def priority(self, error):\n",
    "        \"\"\" Compute an experience priority, as per Schaul et al.\n",
    "        \"\"\"\n",
    "        return (error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\" Current Buffer Occupation\n",
    "        \"\"\"\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Sample a batch, optionally with (PER)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "\n",
    "        # Sample using prorities\n",
    "        if(self.with_per):\n",
    "            T = self.buffer.total() // batch_size\n",
    "            for i in range(batch_size):\n",
    "                a, b = T * i, T * (i + 1)\n",
    "                s = random.uniform(a, b)\n",
    "                idx, error, data = self.buffer.get(s)\n",
    "                batch.append((*data, idx))\n",
    "            idx = np.array([i[5] for i in batch])\n",
    "        # Sample randomly from Buffer\n",
    "        elif self.count < batch_size:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Return a batch of experience\n",
    "        s_batch = np.array([i[0] for i in batch])\n",
    "        a_batch = np.array([i[1] for i in batch])\n",
    "        r_batch = np.array([i[2] for i in batch])\n",
    "        d_batch = np.array([i[3] for i in batch])\n",
    "        new_s_batch = np.array([i[4] for i in batch])\n",
    "        return s_batch, a_batch, r_batch, d_batch, new_s_batch, idx\n",
    "\n",
    "    def update(self, idx, new_error):\n",
    "        \"\"\" Update priority for idx (PER)\n",
    "        \"\"\"\n",
    "        self.buffer.update(idx, self.priority(new_error))\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\" Clear buffer / Sum Tree\n",
    "        \"\"\"\n",
    "        if(self.with_per): self.buffer = SumTree(buffer_size)\n",
    "        else: self.buffer = deque()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \"\"\" Deep Deterministic Policy Gradient (DDPG) Helper Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, batch_no):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        # Environment and A2C parameters\n",
    "        self.state_size = (batch_no,) + state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.00005\n",
    "        # Create actor and critic networks\n",
    "        self.actor = Actor(state_size, self.action_size, 0.1 * self.learning_rate, 0.001)\n",
    "        self.critic = Critic(state_size, self.action_size, self.learning_rate, 0.001)\n",
    "        self.buffer = MemoryBuffer(20000)\n",
    "\n",
    "    def policy_action(self, s):\n",
    "        \"\"\" Use the actor to predict value\n",
    "        \"\"\"\n",
    "        return self.actor.predict(s)[0]\n",
    "\n",
    "    def bellman(self, rewards, q_values, dones):\n",
    "        \"\"\" Use the Bellman Equation to compute the critic target\n",
    "        \"\"\"\n",
    "        critic_target = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                critic_target[i] = rewards[i]\n",
    "            else:\n",
    "                critic_target[i] = rewards[i] + self.gamma * q_values[i]\n",
    "        return critic_target\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state):\n",
    "        \"\"\" Store experience in memory buffer\n",
    "        \"\"\"\n",
    "        self.buffer.memorize(state, action, reward, done, new_state)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return self.buffer.sample_batch(batch_size)\n",
    "\n",
    "    def update_models(self, states, actions, critic_target):\n",
    "        \"\"\" Update actor and critic networks from sampled experience\n",
    "        \"\"\"\n",
    "        # Train critic\n",
    "        self.critic.train_on_batch(states, actions, critic_target)\n",
    "        # Q-Value Gradients under Current Policy\n",
    "        actions = self.actor.model.predict(states)\n",
    "        grads = self.critic.gradients(states, actions)\n",
    "        # Train actor\n",
    "        self.actor.train(states, actions, np.array(grads).reshape((-1, self.action_size)))\n",
    "        # Transfer weights to target networks at rate Tau\n",
    "        self.actor.transfer_weights()\n",
    "        self.critic.transfer_weights()\n",
    "\n",
    "    def train(self, env, render, batch_size, nb_episodes):\n",
    "        results = []\n",
    "\n",
    "        # First, gather experience\n",
    "        # tqdm_e = tqdm(range(nb_episodes), desc='Score', leave=True, unit=\" episodes\")\n",
    "        for e in range(nb_episodes):\n",
    "\n",
    "            # Reset episode\n",
    "            time, cumul_reward, done = 0, 0, False\n",
    "            old_state = env.reset()\n",
    "            actions, states, rewards = [], [], []\n",
    "            noise = OrnsteinUhlenbeckProcess(size=self.action_size, theta=.15, mu=0., sigma=.3)\n",
    "\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                # Actor picks an action (following the deterministic policy)\n",
    "                a = self.policy_action(old_state)\n",
    "                # Clip continuous values to be valid w.r.t. environment\n",
    "                # !!!!!!!!TODO: Adjust act_range ! --> = Output range\n",
    "                a = np.clip(a+noise.sample(), -1, 1)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                new_state, r, done, _ = env.step(a)\n",
    "                # Add outputs to memory buffer\n",
    "                self.memorize(old_state, a, r, done, new_state)\n",
    "                # Sample experience from buffer\n",
    "                states, actions, rewards, dones, new_states, _ = self.sample_batch(batch_size)\n",
    "                # Predict target q-values using target networks\n",
    "                q_values = self.critic.target_predict([new_states, self.actor.target_predict(new_states)])\n",
    "                # Compute critic target\n",
    "                critic_target = self.bellman(rewards, q_values, dones)\n",
    "                # Train both networks on sampled batch, update target networks\n",
    "                self.update_models(states, actions, critic_target)\n",
    "                # Update current state\n",
    "                old_state = new_state\n",
    "                cumul_reward += r\n",
    "                if time % 10 == 0:\n",
    "                    print(\"{} | Action: {}, Reward: {}\".format(time, a, cumul_reward))\n",
    "                    env.render()\n",
    "                time += 1\n",
    "                \n",
    "                self.save_weights('')\n",
    "\n",
    "            # Export results for Tensorboard\n",
    "            # score = tfSummary('score', cumul_reward)\n",
    "            # summary_writer.add_summary(score, global_step=e)\n",
    "            # summary_writer.flush()\n",
    "            # Display score\n",
    "            # tqdm_e.set_description(\"Score: \" + str(cumul_reward))\n",
    "            # tqdm_e.refresh()\n",
    "            print(\"Score: \" + str(cumul_reward))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_weights(self, path):\n",
    "        path += '_LR_{}'.format(self.learning_rate)\n",
    "        self.actor.save(path)\n",
    "        self.critic.save(path)\n",
    "\n",
    "    def load_weights(self, path_actor, path_critic):\n",
    "        self.critic.load_weights(path_critic)\n",
    "        self.actor.load_weights(path_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1077..1357 -> 280-tiles track\n",
      "[[-0.09278685  1.          0.9980209 ]]\n",
      "0 | Action: [-0.12104602  0.97957786  0.97453635], Reward: 7.068458781362008\n",
      "[[0.09420323 1.         0.9995526 ]]\n",
      "[[0.94554555 1.         0.9999995 ]]\n",
      "[[0.71865726 1.         0.99999595]]\n",
      "[[0.8026937  1.         0.99999356]]\n",
      "[[0.30200505 1.         0.9999974 ]]\n",
      "[[0.13756478 1.         0.9999833 ]]\n",
      "[[0.18746448 1.         0.99998903]]\n",
      "[[0.27620578 1.         0.99997354]]\n",
      "[[0.5174556 1.        0.9999778]]\n",
      "[[0.31693494 1.         0.99999094]]\n",
      "10 | Action: [0.28465445 1.         1.        ], Reward: 6.068458781362011\n",
      "[[0.2257489  1.         0.99999523]]\n",
      "[[0.50717187 1.         0.9999945 ]]\n",
      "[[0.19910288 1.         0.9999969 ]]\n",
      "[[0.09442377 1.         0.9999969 ]]\n",
      "[[0.01233649 1.         0.9999974 ]]\n",
      "[[-0.10629064  1.          0.9999957 ]]\n",
      "[[0.04107261 1.         0.99999595]]\n",
      "[[0.04636526 1.         0.99999523]]\n",
      "[[0.19844532 1.         0.9999964 ]]\n",
      "[[0.05767453 1.         0.9999981 ]]\n",
      "20 | Action: [-0.08260801  1.          0.95908418], Reward: 5.068458781362015\n",
      "[[0.2138933  1.         0.99999905]]\n",
      "[[0.09479129 1.         0.99999857]]\n",
      "[[0.15349579 1.         0.99999833]]\n",
      "[[0.24079442 1.         0.99999833]]\n",
      "[[0.33083296 1.         0.99999785]]\n",
      "[[0.29491436 1.         0.9999974 ]]\n",
      "[[0.302446  1.        0.9999988]]\n",
      "[[0.3639115 1.        0.9999976]]\n",
      "[[0.27181268 1.         0.99999785]]\n",
      "[[0.11806858 1.         0.99999714]]\n",
      "30 | Action: [-0.0571068   1.          0.93028307], Reward: 4.068458781362018\n",
      "[[0.037045   1.         0.99999785]]\n",
      "[[0.04383278 1.         0.9999976 ]]\n",
      "[[-0.02988833  1.          0.9999969 ]]\n",
      "[[-0.1324473   1.          0.99999714]]\n",
      "[[-0.15756118  1.          0.9999974 ]]\n",
      "[[-0.21611243  1.          0.9999976 ]]\n",
      "[[-0.09044713  1.          0.9999974 ]]\n",
      "[[0.0638808  1.         0.99999666]]\n",
      "[[0.12021565 1.         0.9999962 ]]\n",
      "[[0.190647   1.         0.99999666]]\n",
      "40 | Action: [0.03497951 1.         0.89861967], Reward: 3.0684587813620174\n",
      "[[0.18951833 1.         0.9999957 ]]\n",
      "[[0.37418556 1.         0.99999475]]\n",
      "[[0.21478677 1.         0.9999933 ]]\n",
      "[[0.17091167 1.         0.99999046]]\n",
      "[[0.05940413 1.         0.99999094]]\n",
      "[[-0.07814729  1.          0.99999356]]\n",
      "[[-0.3540961  1.         0.99999  ]]\n",
      "[[-0.43788648  1.          0.9999919 ]]\n",
      "[[-0.5264169  1.         0.9999883]]\n",
      "[[-0.53170025  1.          0.9999883 ]]\n",
      "50 | Action: [-0.68738944  1.          0.79395096], Reward: 2.0684587813620166\n",
      "[[-0.48542202  1.          0.9999893 ]]\n",
      "[[-0.44648635  1.          0.9999893 ]]\n",
      "[[-0.43916726  1.          0.99998856]]\n",
      "[[-0.38224512  1.          0.9999883 ]]\n",
      "[[-0.36429304  1.          0.99998856]]\n",
      "[[-0.39393288  1.          0.9999881 ]]\n",
      "[[-0.39468402  1.          0.99998975]]\n"
     ]
    }
   ],
   "source": [
    "############## Training #################\n",
    "\n",
    "ddpg = DDPG(env.observation_space.shape, 3, 10)\n",
    "\n",
    "'''\n",
    "for episode in range(2):\n",
    "    score=0\n",
    "    done=False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = ddpg.policy_action(state)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        ddpg.memorize(observation, action, reward, done)\n",
    "        env.render()\n",
    "        score+=reward\n",
    "        state = observation\n",
    "    ddpg.train()\n",
    "    print(\"episode {} score {}\".format(episode, score))\n",
    "'''\n",
    "ddpg.load_weights('_LR_5e-05_actor.h5', '_LR_5e-05_critic.h5')\n",
    "ddpg.train(env, render=False, batch_size=100, nb_episodes=100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktionen:\n",
    "softmax = summe der outputs wird 1 (w'keit)\n",
    "\n",
    "relu = größer null bis unendlich (max)\n",
    "\n",
    "linear = linear einfach, also outputs zwischen -/+ undendlich\n",
    "\n",
    "sigmoid = jeder output zwischen 0 und 1 --> wkeit\n",
    "\n",
    "tanh = zwischen -1 und 1 = mit vorzeichen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based / model-free:\n",
    "--> Two approaches to learn the transition function & reward function\n",
    "\n",
    "Model based: Agent exploits the environment to learn. Model = the environment learned from observations.\n",
    "\n",
    "Model free: Agent relies on trail-and-error experience and doesn't learn the environment. E.g. estimating the optimal values of each action / state. (Q-Learning, actor critic) --> Here it cannot make predictions on the next state before taking an action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value / policy learning:\n",
    "_Policy based_: Give an output given a particular input. (Actor part of actor critic)\n",
    "\n",
    "_Value based_: Assigns a score to the state by calculating the cumulative score for a state. Through a marcov decision process maximize the reward. Actions that result in a greater reward are better. (Q-Learning)\n",
    "\n",
    "\n",
    "-> Advantage of policy based: (for us)\n",
    "- continous action space\n",
    "- stochastic policies (action X for 30%...)\n",
    "\n",
    "-> Advantage of value based: (for us)\n",
    "- simpler\n",
    "- faster\n",
    "\n",
    "\n",
    "--> We use both approaches together in the actor critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sonstiges:\n",
    "- Discount factor wird benutzt um zu verhindern dass der Reward unendlich wird\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erklärung Actor-Critic / DDPG :\n",
    "Deep-Q-Nets können nur für diskreten Input/Output verwendet werden, da ein Input auf die Q-Werte gemappt wird. Bei einem kontinuierlichen Space kann aber solches Mapping nicht stattfinden. \n",
    "Im erstes Schritt, um dieses Problem zu lösen, kann der Actor-Critic Ansatz verwendet werden. Hierbei wird das DQN aufgeteilt in einen actor, der die Aktionen ausführt und einen critic der die Aktionen + Zustände mit Q-Werten bewertet. Damit wird die Q-Wert-Logik vom kontinuierlichen Raum \"abgekoppelt\". Actor-Critic ist modell-free und value based und repräsentiert die policy als parametrische Wahrscheinlichkeitsverteilung. (Stochastisch)\n",
    "Eine weitere Verbesserung ist der Deep Determninistic Policy Gradient (DDPG), da dieser eine bessere Performance als der stochastic actor critic ansatz (SAC) hat. Allerdings hemmt dieser ansatz die Exploration und daher muss ein off-policy Algorithmus (und noise) implementiert werden, der den state-action space exploriert. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Zusammenfassung (DDPG)\n",
    "\n",
    "#### General\n",
    "- policy gradient funktioniert im kontinuierlichen weil er quasi eine Funktion ist (damit kontinuierlich)\n",
    "- Das paper basiert auf diesem kontinuierlichen Modell und erweitert es durch ein DQN \n",
    "- DQN heißt so, weil es statt einer Tabelle (Actions/States) ein Neuronales Netzt verwendet, welches fähig ist hochdimensionalen Input wie z.B. Bilder zu mappen\n",
    "- Hautproblem: Nur diskrete Action-Spaces bei DQN\n",
    "- Eine Möglichkeit ist die Diskretisierung des Action Spaces, dies führt aber zu sehr vielen möglichen Aktionen > schlecht, weil schwer zu explorieren\n",
    "- Warum wird ein DQN verwendet? Warum ist es ein Sinnvoller Approximator?\n",
    "1. Off policy (nach dem Training) with replay buffer (gegen correlations in den aktionen)\n",
    "2. Network is trained with target Q network + batch normalization\n",
    "- Für die implementierung braucht man: actor-critic architektur, lernalgorithmus mit so wenig Anpassungsmöglichkeiten wie möglich\n",
    "\n",
    "#### Background\n",
    "- Der agent führt eine policy aus, welche Zustände zu eienr Wahrscheinlichkeitsverteilung für Aktionen mappt\n",
    "- Das Ganze is als Markov Decision Process modelliert (aktueller Zustand hängt nur vom vorherigen Zustand ab) und es gibt Übergangswahrscheinlichkeiten, die aber bei model-free nicht bekannt sind. \n",
    "- return of a state = sum of discounted future reward\n",
    "- goal = learn policy with maximizing expected reward\n",
    "- Dafür wird die Bellman equation verwendet\n",
    "- Der Agent muss sich nicht im Environment befinden um zu lernen, sondern es kann nach der Interaktion gelernt werden (es reicht die Reaktionen des Environments zu kennen) --> Daher Q-Learning (off-policy, greedy -> maximaler Q-Wert)\n",
    "- Q_Learning hat ein seperates target network um die Zielwerte zu berechnen --> Dieses verbessert die Stabilität enorm!\n",
    "ggf.: https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4\n",
    "- Loss minimiert den Fehler aus tatsächlichem Q-Wert und vorhergesagtem Q Wert durch die Anpassung des Q-Netzes bzw. dessen Gewichte\n",
    "\n",
    "#### Algorihmus\n",
    "- Actor netz (function), dass die aktuelle policy beinhaltet\n",
    "-  µ(s||θ_µ) --> gegeben dem State und der Policy function (=Actor Netz) wird die policy ausgeführt\n",
    "- Critic netz (DQN), verwendet Bellmann Gleichung\n",
    "- WICHTIG: Actor wird über Kettenregel upgedated. --> Gradient ascent --> Der Actor will \"aufsteigen\" / zum maximum der Q-Werte durch anpassen seiner policy d.h. seines Netzes / gewichte \n",
    "= Entspricht dem Policy Gradient\n",
    "- Die Umformungen sind vermutlich wegen der Kettenregel\n",
    "- Batch learning wird verwendet um die Stabilität zu erhöhen (replay buffer) --> Minibatches resetten die policy nicht und ermöglichen zusätzlich independent & identical distributed samples\n",
    "- Replay buffer:\n",
    "1. Es ist ein \"Speicher\" von vorherigen State-Action-Reward-pairs\n",
    "2. Sample alle tuples (s, a, r, s+1)\n",
    "3. Alte Samples werden \"discarded\" --> Queue in Python\n",
    "4. Nach jeder \"Episode\" wird actor und critic geupdated durch einen minibatch aus random tuples\n",
    "\n",
    "- Target net wird benutzt, da das Critic-Net anfällig ist unstabil zu sein (weil die ziel-Q-werte ebenfalls auf der bellmann gleichung basieren)\n",
    "- Es verwendet daher ein target network mit \"soften\" updates (statt die Gewichte zu kopieren) --> Mehr Literatur nachlesen\n",
    "- Dafür wird eine Kopie der beiden Netze (actor, critic) erstellt und verwendet um die target-werte zu berechnen. Diese Werden geupdated (langsam) \n",
    "- Das \"tau\" bestimmt die Stärke der updates --> In die target netze werden die neuen Netzen zu einem bestimmten prozentsatz tau * Netz miteinbezogen. Target_netz = Tatsächliches_Netz * Tau + Target_netz * (1- Tau)\n",
    "- Dadurch wird das ganze quasi supervised, da man target-werte als Zielwerte hat und die eigenen werte als predictions\n",
    "- Die target-netze hinken dann zwar hinterher, aber da das Problem dadurch stabil wird lohnt es sich\n",
    "- EVENTUELL FÜR CARLA RELEVANT --> Batch normaization um die features zu skalieren, sodass es generisch für die Inputs ist\n",
    "- Normalisiert jede Dimension in einem Minibatch mit mean/variance\n",
    "- Verwendet einen running average of the mean and variance\n",
    "- GGF. später ansehen??? --> Braucht man nur um in verschiedenen Environmens zu lernen\n",
    "- Wichtig: exploration bei Kontinuierlichen Action Spaces ist schwierig, da es unendlich viele möglichkeiten gibt...Hierbei wird bei DDPG unabhängig vom lern-algorithmus exploriert. Exploration is on-policy und learning off-policy\n",
    "- Hierzu wird Noise (Ornstein-Uhlenbeck) hinzugefügt um den kontinuierlichen Raum zu erkunden.\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Model #############\n",
    "class Actor:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        # Build the model layer by layer\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Convolutions\n",
    "        model.add(Conv2D(64, kernel_size=9, activation='relu', input_shape=self.state_size))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(128, kernel_size=9, activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        # Output an der stelle sind 256 features mit jeweils 1/16 der ursprünglichen Bildgröße\n",
    "        model.add(Conv2D(256, kernel_size=9, activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Connect convolution and dense layers\n",
    "        # 2D -> 1D (Linearization)\n",
    "        model.add(Flatten())\n",
    "    \n",
    "        # 3 hidden layers\n",
    "        # This part is where the actual learning happens\n",
    "        # 2 layers are sufficient to learn everything\n",
    "        # Creates 9216 x 512 weights\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        # Creates 512 x 512 weights\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        \n",
    "        # Output neurons (number of actions) (512 x 3)\n",
    "        model.add(Dense(self.action_size, activation='sigmoid'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "class Critic:\n",
    "    '''\n",
    "    Set basic parameters for the model\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    '''\n",
    "    Build a convolutional neural net with 3 output neurons\n",
    "    '''\n",
    "    def _build_model(self):\n",
    "        # Build the model layer by layer\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Convolutions\n",
    "        model.add(Conv2D(64, kernel_size=9, activation='relu', input_shape=self.state_size))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(128, kernel_size=9, activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        # Output an der stelle sind 256 features mit jeweils 1/16 der ursprünglichen Bildgröße\n",
    "        model.add(Conv2D(256, kernel_size=9, activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Connect convolution and dense layers\n",
    "        # 3D -> 1D (Linearization)\n",
    "        model.add(Flatten())\n",
    "    \n",
    "        # 3 hidden layers\n",
    "        # This part is where the actual learning happens\n",
    "        # 2 layers are sufficient to learn everything\n",
    "        # Creates 9216 x 512 weights\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        # Creates 512 x 512 weights\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        \n",
    "        # Output neurons (number of actions) (512 x 3)\n",
    "        model.add(Dense(self.action_size, activation='sigmoid'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "        \n",
    "actor = Actor(env.observation_space.shape, 3)\n",
    "critic = Critic(env.observation_space.shape, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Params ###########\n",
    "episodes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# FIRST: Continous control verstehen!!!!!!!!!! How is it done?\n",
    "# erstmal das argmax nochmal ansehen... das sind die q werte! davon den höchsten\n",
    "# Das mapping mit -1 bis 1 macht hier gar keinen sinn --> das muss später kommen\n",
    "# Zum anfang kann man mal alles mit 0 oder 1 machen für die aktionen\n",
    "\n",
    "for episode in range(episodes):\n",
    "    score=0\n",
    "    done=False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        state = state.reshape(1,96,96,3)\n",
    "        action = agent.act(state)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation = observation.reshape(1,96,96,3)\n",
    "        agent.remember(state, action, reward, observation, done)\n",
    "        state = observation\n",
    "        env.render()\n",
    "        score+=reward\n",
    "    agent.replay(32)\n",
    "    print(\"episode {} score {} exploration {}\".format(episode, score, agent.epsilon))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Kill ###########\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_project_env",
   "language": "python",
   "name": "rl_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
